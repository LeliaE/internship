# Oticon Medical Research and Technology Internship

## Abstract
Cochlear implant (CI) can restore sensation of hearing and near-perfect speech communication to many patients with severe-to-profound sensorineural hearing loss in quiet environments. However, many challenges remain. There are big individual variabilities observed in speech and quality of life outcomes following cochlear implantation. Specifically, most of the clinical assessments for patients are conducted in an isolated lab or clinic, using unrealistic sound stimuli, which are far from the real communication scenarios CI users face daily. Therefore, results generated from these tests cannot uncover fully the challenges and difficulties CI users are experiencing. This discrepancy might explain the lack of association between real-life quality-of-life measurement and standard clinical assessment. Oticon Medical Field Research Platform (OMFRP) introduces a new way to investigate the realistic usage of medical devices for CI users. It is a standalone iPhone application connected with the hearing devices via Bluetooth connection to control device-related settings (e.g., programs, sound level, data logging, acoustic scene classification). The application interacts with the user by collecting ecological momentary assessment (EMA) questionnaires at certain intervals. Our research aimes to examine the effect of soundscapes on cognitive effort experienced by CI users.  By combining machine learning algorithms and experience sampling methods, our findings could lead to improved automated tuning capabilities within CIs, addressing unique listening demands across diverse conditions encountered by CI recipients. We designed a machine learning model for predicting acoustic scenes in complex environments, iterating through various architectures and input auditory features to achieve optimal performance while maintaining a lightweight footprint suitable for embedding on resource-constrained platforms like smartphones. Ultimately, customized device functionality should reduce user burden and enhance overall well-being when interacting with surrounding sonic environments. Cochlear implant (CI) can restore sensation of hearing and near-perfect speech communication to many patients with severe-to-profound sensorineural hearing loss in quiet environments. However, many challenges remain. There are big individual variabilities observed in speech and quality of life outcomes following cochlear implantation. Specifically, most of the clinical assessments for patients are conducted in an isolated lab or clinic, using unrealistic sound stimuli, which are far from the real communication scenarios CI users face daily. Therefore, results generated from these tests cannot uncover fully the challenges and difficulties CI users are experiencing. This discrepancy might explain the lack of association between real-life quality-of-life measurement and standard clinical assessment. Oticon Medical Field Research Platform (OMFRP) introduces a new way to investigate the realistic usage of medical devices for CI users. It is a standalone iPhone application connected with the hearing devices via Bluetooth connection to control device-related settings (e.g., programs, sound level, data logging, acoustic scene classification). The application interacts with the user by collecting ecological momentary assessment (EMA) questionnaires at certain intervals. Our research aimes to examine the effect of soundscapes on cognitive effort experienced by CI users.  By combining machine learning algorithms and experience sampling methods, our findings could lead to improved automated tuning capabilities within CIs, addressing unique listening demands across diverse conditions encountered by CI recipients. We designed a machine learning model for predicting acoustic scenes in complex environments, iterating through various architectures and input auditory features to achieve optimal performance while maintaining a lightweight footprint suitable for embedding on resource-constrained platforms like smartphones. Ultimately, customized device functionality should reduce user burden and enhance overall well-being when interacting with surrounding sonic environments. 

## Data
[DEMAND: Diverse Environments Multichannel Acoustic Noise Database](https://zenodo.org/record/1227121)  
[TAU Urban Acoustic Scenes 2022 Mobile, Development dataset](https://zenodo.org/record/6337421)  
[LibriSpeech ASR corpus](https://www.openslr.org/12)  
[ESC-50: Dataset for Environmental Sound Classification](https://github.com/karolpiczak/ESC-50)  
[Deeply parent-child vocal interaction dataset](https://www.openslr.org/98/)  

## References
[1] Wang, Z., Chang, Y., Schmeichel, B. J., & Garcia, A. A. (2022). The effects of mental fatigue on effort allocation: Modeling and estimation. Psychological review, 129(6), 1457–1485. https://doi.org/10.1037/rev0000365  
[2] Dingemanse, G., & Goedegebure, A. (2022). Listening Effort in Cochlear Implant Users: The Effect of Speech Intelligibility, Noise Reduction Processing, and Working Memory Capacity on the Pupil Dilation Response. Journal of speech, language, and hearing research : JSLHR, 65(1), 392–404. https://doi.org/10.1044/2021_JSLHR-21-00230  
[3] Holube, I., von Gablenz, P., & Bitzer, J. (2020). Ecological Momentary Assessment in Hearing Research: Current State, Challenges, and Future Directions. Ear and hearing, 41 Suppl 1, 79S–90S. https://doi.org/10.1097/AUD.0000000000000934  
[4] J. Thiemann, N. Ito, and E. Vincent, “DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments,” June 2013. Supported by Inria under the Associate Team Program VERSAMUS.  
[5] T. Heittola, A. Mesaros, and T. Virtanen, “TAU Urban Acoustic Scenes 2019, Development dataset,” Mar. 2019.  
[6] D. Inc., “Deeply parent-child vocal interaction dataset,” 2021  
[7] K. J. Piczak, “ESC: Dataset for Environmental Sound Classification,” in Proceedings of the 23rd Annual ACM Conference on Multimedia, pp. 1015–1018, ACM Press.  
[8] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 5206–5210, IEEE, 2015  
